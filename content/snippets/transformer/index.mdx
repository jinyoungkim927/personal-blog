---
title: "Transformer"
date: 2026-01-20
tags:
  - ML
  - computer-science
  - data-science
  - optimization
---

### What is it
- Sequece of transformer blocks, making them highly parallelisable
- Key idea is Attention
	- Encoder: unmasked self-attention
	- Decoder: masked self-attention
		- LLMs are only using this decoding part. Just predicting the next word in a sequence. Rather than translation. Given this sequence, predict the next one. 

What are the outputs going in? Aren't we trying to produce the output? 
- You feed in the sequence, can only attend to the tokens in the past. 
- Operations in the past, at the top 
- Argmax and sampling 

### History: 
- GPT-1 shows the pre-training and the fine-tuning paradigm works **really well**
### Computer Vision Version: CLIP
- CLIP is agnostic to choice of architecutre. It groups similar text and image pairs, pushes apart text and image pairs that are different. 
- Vision Transformer 
